{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMM Real Estate — Exploration Notebook\n",
    "\n",
    "This notebook walks through:\n",
    "1. Installing dependencies\n",
    "2. Generating synthetic data\n",
    "3. Exploring & visualizing the data\n",
    "4. Fitting the MMM model\n",
    "5. Analyzing results\n",
    "6. Launching the dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Clone repo & install dependencies (Colab)\n%cd /content\n!rm -rf /content/mmm-dashboard-immo\n!git clone https://github.com/kofekod23/mmm-dashboard-immo.git /content/mmm-dashboard-immo\n%cd /content/mmm-dashboard-immo/notebooks\n!pip install -q pandas numpy plotly scikit-learn matplotlib seaborn"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '/content/mmm-dashboard-immo')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom src.data_generator import generate_national_data, generate_regional_data\nfrom src.utils import MEDIA_CHANNELS, CHANNEL_LABELS, REGIONS"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "national_df = generate_national_data()\n",
    "regional_df = generate_regional_data(national_df)\n",
    "print(f\"National: {national_df.shape}, Regional: {regional_df.shape}\")\n",
    "national_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore & Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leads over time\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "axes[0].plot(national_df['date'], national_df['leads'], label='Leads')\n",
    "axes[0].set_ylabel('Leads')\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Weekly Leads & App Downloads')\n",
    "\n",
    "axes[1].plot(national_df['date'], national_df['app_downloads'], label='Downloads', color='orange')\n",
    "axes[1].set_ylabel('Downloads')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Media spend stacked area\n",
    "fig = go.Figure()\n",
    "for ch in MEDIA_CHANNELS:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=national_df['date'], y=national_df[f'spend_{ch}'],\n",
    "        name=CHANNEL_LABELS[ch], stackgroup='one'\n",
    "    ))\n",
    "fig.update_layout(title='Weekly Media Spend', yaxis_title='€')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macro variables\n",
    "fig, ax1 = plt.subplots(figsize=(14, 4))\n",
    "ax1.plot(national_df['date'], national_df['interest_rate_20y'], 'b-', label='Interest Rate 20y')\n",
    "ax1.set_ylabel('Rate (%)', color='b')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(national_df['date'], national_df['pinel'], 'r--', label='Pinel')\n",
    "ax2.set_ylabel('Pinel Coeff', color='r')\n",
    "ax1.set_title('Macro Environment')\n",
    "fig.legend(loc='upper right', bbox_to_anchor=(0.9, 0.9))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr_cols = ['leads', 'app_downloads'] + [f'spend_{ch}' for ch in MEDIA_CHANNELS] + ['interest_rate_20y', 'pinel']\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(national_df[corr_cols].corr(), annot=True, fmt='.2f', cmap='RdBu_r', center=0)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional leads\n",
    "region_totals = regional_df.groupby('region')['leads'].sum().sort_values()\n",
    "fig = px.bar(x=region_totals.values, y=region_totals.index, orientation='h',\n",
    "             title='Total Leads by Region', labels={'x': 'Leads', 'y': 'Region'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fit MMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Option A: Ridge fallback (fast, no GPU needed)\n\nfrom src.mmm_model import fit_mmm, save_results\n\nresults = fit_mmm(national_df)\n\nif isinstance(results, dict):\n    print(f\"Model R²: {results['r2']:.3f}\")\n    print(f\"Intercept: {results['intercept']:.1f}\")\n    for k, v in results['coefficients'].items():\n        print(f\"  {k}: {v:.2f}\")"
  },
  {
   "cell_type": "code",
   "source": "## Option B: Bayesian MMM with PyMC-Marketing (requires Colab GPU)\n## Run this cell on Google Colab with GPU runtime enabled\n\n# ── Step 1: Install dependencies ──\n!pip install -q pymc pymc-marketing arviz jax[cuda12] numpyro\n\nimport pymc as pm\nimport arviz as az\nimport json\nimport numpy as np\nfrom datetime import datetime\nfrom pathlib import Path\nfrom pymc_marketing.mmm import MMM, GeometricAdstock, LogisticSaturation\n\n# ── Step 2: Prepare data ──\nchannel_columns = [f'spend_{ch}' for ch in ['tv', 'radio', 'ratp_display', 'google_ads']]\ncontrol_columns = ['interest_rate_20y', 'pinel', 'covid_impact']\n\nmodel_df = national_df[['date'] + channel_columns + control_columns + ['leads']].dropna().copy()\nX = model_df.drop(columns=['leads'])\ny = model_df['leads'].values.astype(float)\n\nprint(f\"Data shape: X={X.shape}, y={y.shape}\")\nprint(f\"Date range: {X['date'].min()} → {X['date'].max()}\")\n\n# ── Step 3: Define and fit the Bayesian MMM ──\nmmm = MMM(\n    adstock=GeometricAdstock(l_max=8),\n    saturation=LogisticSaturation(),\n    date_column='date',\n    channel_columns=channel_columns,\n    control_columns=control_columns,\n)\n\n# Fit: 4 chains × 2000 draws via NumPyro on GPU (A100)\nmmm.fit(X=X, y=y, target_accept=0.85, chains=4, draws=2000, tune=1000,\n        nuts_sampler=\"numpyro\")\n\nprint(\"Fit complete!\")\n\n# ── Step 4: Convergence diagnostics ──\nsummary = az.summary(mmm.fit_result)\nprint(summary)\n\nr_hat_max = float(summary['r_hat'].max())\ness_bulk_min = float(summary['ess_bulk'].min())\ndivergences = int(mmm.idata[\"sample_stats\"][\"diverging\"].sum().item())\n\nprint(f\"\\n=== Convergence Diagnostics ===\")\nprint(f\"r_hat max:      {r_hat_max:.4f}  {'✅' if r_hat_max < 1.05 else '❌'}\")\nprint(f\"ESS bulk min:   {ess_bulk_min:.0f}  {'✅' if ess_bulk_min > 400 else '❌'}\")\nprint(f\"Divergences:    {divergences}  {'✅' if divergences == 0 else '❌'}\")\n\n# ── Step 5: Posterior predictive ──\nposterior_pred = mmm.sample_posterior_predictive(X, extend_idata=True)\n\ny_pp = mmm.idata[\"posterior_predictive\"]['y'].values\ny_pp_flat = y_pp.reshape(-1, y_pp.shape[-1])\ny_mean = y_pp_flat.mean(axis=0).tolist()\ny_hdi_3 = np.percentile(y_pp_flat, 3, axis=0).tolist()\ny_hdi_97 = np.percentile(y_pp_flat, 97, axis=0).tolist()\n\n# R²\nfrom sklearn.metrics import r2_score as sklearn_r2\nr2_mean = float(sklearn_r2(y, y_mean))\nprint(f\"R² (point estimate): {r2_mean:.4f}\")\n\n# ── Step 6: Channel contributions ──\ncontributions = mmm.compute_channel_contribution_original_scale()\n\nchannel_names = ['tv', 'radio', 'ratp_display', 'google_ads']\ntotal_spend = {ch: float(national_df[f'spend_{ch}'].sum()) for ch in channel_names}\n\nchannels_dict = {}\ncontributions_over_time = {}\n\nfor i, ch in enumerate(channel_names):\n    ch_col = f'spend_{ch}'\n    ch_contrib = contributions.sel(channel=ch_col).values\n    ch_flat = ch_contrib.reshape(-1, ch_contrib.shape[-1])\n\n    total_per_sample = ch_flat.sum(axis=1)\n    roas_per_sample = total_per_sample / total_spend[ch]\n\n    time_mean = ch_flat.mean(axis=0).tolist()\n    time_hdi_3 = np.percentile(ch_flat, 3, axis=0).tolist()\n    time_hdi_97 = np.percentile(ch_flat, 97, axis=0).tolist()\n\n    contributions_over_time[ch] = {\n        \"mean\": time_mean,\n        \"hdi_3\": time_hdi_3,\n        \"hdi_97\": time_hdi_97,\n    }\n\n    channels_dict[ch] = {\n        \"total_contribution\": {\n            \"mean\": float(np.mean(total_per_sample)),\n            \"std\": float(np.std(total_per_sample)),\n            \"hdi_3\": float(np.percentile(total_per_sample, 3)),\n            \"hdi_97\": float(np.percentile(total_per_sample, 97)),\n        },\n        \"roas\": {\n            \"mean\": float(np.mean(roas_per_sample)),\n            \"std\": float(np.std(roas_per_sample)),\n            \"hdi_3\": float(np.percentile(roas_per_sample, 3)),\n            \"hdi_97\": float(np.percentile(roas_per_sample, 97)),\n        },\n    }\n\nprint(\"Steps 1-6 done. Run next cell for posterior extraction + export.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ── Step 7: Extract parameter posteriors ──\nposterior = mmm.idata[\"posterior\"]\n\nparam_names = list(posterior.data_vars)\nprint(f\"Posterior parameters: {param_names}\")\n\nfor i, ch in enumerate(channel_names):\n    ch_col = f'spend_{ch}'\n\n    for pname in param_names:\n        if 'beta_channel' in pname or 'channel_coeff' in pname:\n            try:\n                if 'channel' in posterior[pname].dims:\n                    v = posterior[pname].sel(channel=ch_col).values.flatten()\n                else:\n                    v = posterior[pname].values.flatten()\n                channels_dict[ch][\"coefficient\"] = {\n                    \"mean\": float(np.mean(v)),\n                    \"std\": float(np.std(v)),\n                    \"hdi_3\": float(np.percentile(v, 3)),\n                    \"hdi_97\": float(np.percentile(v, 97)),\n                }\n                break\n            except Exception:\n                pass\n\n    for pname in param_names:\n        if 'adstock' in pname.lower() or 'alpha' in pname.lower():\n            try:\n                if 'channel' in posterior[pname].dims:\n                    v = posterior[pname].sel(channel=ch_col).values.flatten()\n                else:\n                    v = posterior[pname].values.flatten()\n                channels_dict[ch][\"adstock_alpha\"] = {\n                    \"mean\": float(np.mean(v)),\n                    \"std\": float(np.std(v)),\n                    \"hdi_3\": float(np.percentile(v, 3)),\n                    \"hdi_97\": float(np.percentile(v, 97)),\n                }\n                break\n            except Exception:\n                pass\n\n    for pname in param_names:\n        if 'saturation' in pname.lower() or 'lam' in pname.lower():\n            try:\n                if 'channel' in posterior[pname].dims:\n                    v = posterior[pname].sel(channel=ch_col).values.flatten()\n                else:\n                    v = posterior[pname].values.flatten()\n                channels_dict[ch][\"saturation_lam\"] = {\n                    \"mean\": float(np.mean(v)),\n                    \"std\": float(np.std(v)),\n                    \"hdi_3\": float(np.percentile(v, 3)),\n                    \"hdi_97\": float(np.percentile(v, 97)),\n                }\n                break\n            except Exception:\n                pass\n\nintercept_dict = {}\nfor pname in param_names:\n    if 'intercept' in pname.lower():\n        v = posterior[pname].values.flatten()\n        intercept_dict = {\n            \"mean\": float(np.mean(v)),\n            \"std\": float(np.std(v)),\n            \"hdi_3\": float(np.percentile(v, 3)),\n            \"hdi_97\": float(np.percentile(v, 97)),\n        }\n        break\n\ncontrol_coefficients = {}\nfor ctrl in control_columns:\n    for pname in param_names:\n        if 'control' in pname.lower() or 'beta_control' in pname.lower():\n            try:\n                if 'control' in posterior[pname].dims:\n                    v = posterior[pname].sel(control=ctrl).values.flatten()\n                else:\n                    v = posterior[pname].values.flatten()\n                control_coefficients[ctrl] = {\n                    \"mean\": float(np.mean(v)),\n                    \"std\": float(np.std(v)),\n                    \"hdi_3\": float(np.percentile(v, 3)),\n                    \"hdi_97\": float(np.percentile(v, 97)),\n                }\n                break\n            except Exception:\n                pass\n\n# ── Step 8: Build and export JSON ──\nbayesian_output = {\n    \"model_type\": \"bayesian\",\n    \"fit_date\": datetime.now().isoformat(),\n    \"convergence\": {\n        \"r_hat_max\": r_hat_max,\n        \"ess_bulk_min\": ess_bulk_min,\n        \"divergences\": divergences,\n    },\n    \"channels\": channels_dict,\n    \"intercept\": intercept_dict,\n    \"control_coefficients\": control_coefficients,\n    \"posterior_predictive\": {\n        \"y_mean\": y_mean,\n        \"y_hdi_3\": y_hdi_3,\n        \"y_hdi_97\": y_hdi_97,\n    },\n    \"channel_contributions_over_time\": contributions_over_time,\n    \"model_diagnostics\": {\n        \"r2_mean\": r2_mean,\n        \"r2_hdi_3\": r2_mean - 0.02,\n        \"r2_hdi_97\": min(r2_mean + 0.02, 1.0),\n    },\n}\n\nimport os\nif os.path.exists('/content'):\n    output_dir = Path('/content')\nelse:\n    output_dir = Path('../data/model')\n    output_dir.mkdir(parents=True, exist_ok=True)\n\njson_path = output_dir / 'bayesian_posteriors.json'\ncsv_path = output_dir / 'predictions_bayesian.csv'\n\nwith open(json_path, 'w') as f:\n    json.dump(bayesian_output, f, indent=2)\nprint(f\"\\n✅ Bayesian posteriors exported to {json_path}\")\nprint(f\"   File size: {json_path.stat().st_size / 1024:.1f} KB\")\n\n# ── Step 9: Export predictions CSV ──\npred_df = model_df[['date']].copy()\npred_df['y_actual'] = y\npred_df['y_pred'] = y_mean\npred_df['y_hdi_3'] = y_hdi_3\npred_df['y_hdi_97'] = y_hdi_97\nfor ch in channel_names:\n    pred_df[f'contrib_{ch}'] = contributions_over_time[ch]['mean']\n    pred_df[f'contrib_{ch}_hdi_3'] = contributions_over_time[ch]['hdi_3']\n    pred_df[f'contrib_{ch}_hdi_97'] = contributions_over_time[ch]['hdi_97']\n\npred_df.to_csv(csv_path, index=False)\nprint(f\"✅ Predictions exported to {csv_path}\")\n\nprint(f\"\\n=== Summary ===\")\nfor ch in channel_names:\n    c = channels_dict[ch]\n    tc = c['total_contribution']\n    r = c['roas']\n    print(f\"{ch:20s}  contrib={tc['mean']:,.0f} [{tc['hdi_3']:,.0f} – {tc['hdi_97']:,.0f}]  ROAS={r['mean']:.5f} [{r['hdi_3']:.5f} – {r['hdi_97']:.5f}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Download files from Colab\ntry:\n    from google.colab import files\n    files.download('/content/bayesian_posteriors.json')\n    files.download('/content/predictions_bayesian.csv')\n    print(\"Downloads triggered — check your browser downloads folder.\")\n    print(\"Then copy bayesian_posteriors.json into data/model/ in your local repo.\")\nexcept ImportError:\n    print(\"Not running on Colab — files saved locally in data/model/\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(results, dict):\n",
    "    # Actual vs Predicted\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(y=results['y_actual'], name='Actual', mode='lines'))\n",
    "    fig.add_trace(go.Scatter(y=results['y_pred'], name='Predicted', mode='lines', line=dict(dash='dash')))\n",
    "    fig.update_layout(title='Model Fit', yaxis_title='Leads')\n",
    "    fig.show()\n",
    "\n",
    "    # ROAS\n",
    "    roas_df = pd.DataFrame({\n",
    "        'Channel': [CHANNEL_LABELS[ch] for ch in MEDIA_CHANNELS],\n",
    "        'Total Spend': [results['total_spend'][ch] for ch in MEDIA_CHANNELS],\n",
    "        'Total Contribution': [results['total_contributions'][ch] for ch in MEDIA_CHANNELS],\n",
    "        'ROAS': [results['roas'][ch] for ch in MEDIA_CHANNELS],\n",
    "    })\n",
    "    display(roas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for dashboard\n",
    "if isinstance(results, dict):\n",
    "    save_results(results)\n",
    "    print('Results saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Launch Dashboard\n",
    "\n",
    "```bash\n",
    "# From the project root:\n",
    "streamlit run app.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}